Title: Data Challenge #2

Authors: Brandon Wolff, Zachary Heinemann, and Stephanie Langeland

Due date: 3/22/2017

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

__Review/load the data: __

```{r}
rm(list = ls(all = TRUE))   # cleans everything in the workspace

library(readr)          # easier reading of flat files
library(caret)          # classification and regresssion training package

# ::::::: SOME USEFUL DEFINITIONS :::::::::::::::::::::::::::::::::::::::::::::  

# set the general path for the project at its root, specific files will define 
# their own branches individually
# NOTE that specifying your path will be different in Windows

path <- "/Users/StephanieLangeland/Desktop/Columbia/Applied Data Science/Git/QMSS_G5069_Applied_D_S/Data Challenges/datachallenge2" ## Stephanie's path

# path <- "C:\\Users\\Brandon\\Documents\\GitHub\\QMSS_G5069_Applied_D_S\\Data Challenges\\datachallenge2" ## Brandon's path

# path <- "" ## Zach's path

# define additional paths for files you will use. In each case, determine
# appropriate additions to the path

inFileName1   <- "data/processed/AllViolenceData_170216.csv"     # cleaned data on violence
outFileName1  <- "graphs/RF_VarImportance.pdf"       
outFileName2  <- "graphs/RF_MSE.pdf"  

# ::::::: APPLY INITIAL DEFINITIONS ::::::::::::::::::::::::::::::::::::::::::: 

# set your path to that defined above, and confirm it
setwd(path)   
getwd()

# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: 
# :::::::::::::::::::::: LOADS DATA :::::::::::::::::::::::::::::::::::::::::::  


# ::::::: LOADING RAW DATA
# note that read_csv() guesses column types, so that date is read as a date 
# very useful for plotting time series

AllData <- read_csv(inFileName1) 

# rough validations that data was correctly loaded
## names(AllData)
## nrow(AllData)
## summary(AllData)

## The authories involved in each event are noted in their own columns in the 
## dataset.  For example when `AllData$army == 1` the army was involved.
## Guide to authority lookup:
auth_lookup <- "data/external/LookupAuthorityNames.csv"
auth_lookup_data <- read.csv(auth_lookup)
```

This is a team assignment. Please create a file on your team GitHub repo where 
you answer the challenge, including links to your code, and graphs.

# Model #1 for question #1:

__1.)__	Ask two (2) questions that might help you understand better the dynamics of 
violence contained on our dataset. Apply one algorithm per question and share 
your insights from each analysis. [50 pts] Remember: a non-finding is also a 
finding! It tells you whether a question is worth pursuing further or not.
  
  * Which type of regression model (linear, quadratic, cubic, quartic) best 
  predicts whether or not people will be detained using 
  all weapons (defined as `clips.seized`, `cartridge.sezied`, `small.arms.seized`, and
  `long.guns.seized`) seized and wounded organized crime members as predictors?
  Will this model help the Mexican government formulate a strategy when the goal
  is to maximize the number of people detained? 

__1a)__	perform the necessary transformations in your data - if any are needed, 
and explain why you did that

  * The variables `clips.seized`, `cartridge.sezied`, `small.arms.seized`, and
  `long.guns.seized` will be summed into one variable named `weapons_seized`.
  This new variable will allow us to analyze the total number of weapons seized.
  We want to understand how seizing any type of weapon relates to 
  whether people are detained rather than the number of each type of weapon
  seized.  We are including `clips.seized` because clips are part of weapons.
  
  * A new variable will be created named `detained_resp` to record whether people 
  where detained (`1`) or not (`0`).  We want to predict the response of whether
  people will be detained or not rather than the number of people detained.
  
```{r}
AllData$weapons_seized <- transform(AllData$clips.seized + 
                                      AllData$cartridge.sezied +
                                      AllData$small.arms.seized +
                                      AllData$long.guns.seized)

typeof(AllData$weapons_seized)
AllData$weapons_seized <- as.numeric(unlist((AllData$weapons_seized))) # convert 
# from list to numeric for regression models

AllData$detained_resp <- ifelse(AllData$detained > 0, 1, 0) 

typeof(AllData)
AllData <- as.data.frame(AllData) # convert from list to data.frame
```

__1b)__	show the output from your analysis in a consumable form
  
  * __Regression Models:__
  
```{r}
set.seed(1234)  # for reproducibility 
train <- sample(nrow(AllData), 2698) # randomly sample half of the data
training <- AllData[train, ] # create training set
testing <- AllData[-train, ] # create testing set

linear <- lm(detained_resp ~ weapons_seized + organized.crime.wounded, 
             data = training) # linear regression 

quadratic <- lm(detained_resp ~ poly(weapons_seized + organized.crime.wounded, degree = 2),
                data = training) # quadratic regression
 
cubic <- lm(detained_resp ~ poly(weapons_seized + organized.crime.wounded, degree = 3),
            data = training) # cubic regression

quartic <- lm(detained_resp ~ poly(weapons_seized + organized.crime.wounded, degree = 4),
              data = training) # quartic regression

p_linear <- predict(linear, newdata = testing, type = "response") # predict the repsonse 
p_liner_table <- confusionMatrix(testing$detained_resp, 
                                 as.integer(p_linear > 0.5)) # confusion matrix
 
p_quadratic <- predict(quadratic, newdata = testing, type = "response")
p_quadratic_table <- confusionMatrix(testing$detained_resp, as.integer(p_quadratic > 0.5)) 

p_cubic <- predict(cubic, newdata = testing, type = "response")
p_cubic_table <- confusionMatrix(testing$detained_resp, as.integer(p_cubic > 0.5))

p_quartic <- predict(quartic, newdata = testing, type = "response")
p_quartic_table <- confusionMatrix(testing$detained_resp, as.integer(p_quartic > 0.5))
```
  
  * __Regression Results:__
  A confusion matrix was constructed for each of the models and ranked in 
  order from worst to best prediction accuracy:
  
```{r}
p_quadratic_table # quadratic regression confusion matrix 
p_liner_table # linear regression confusion matrix 
p_quartic_table # quartic regression confusion matrix
p_cubic_table # cubic regression confusion matrix 
```
  
  * __Analysis of the output:__
  The cubic regression model had the highest 
  prediction accuracy of 0.7135 or 71.35% in using wounded organized crime members
  and all weapons seized to predict whether people would be detained or not.  The
  cubic model's confusion matrix shows that the model correctly predicted that no one
  would be detained 1,875 times and people would be detained 50 times. Although 
  the model has the highest prediction accuracy, it is mainly driven by correct 
  predictions of no one being detained.  Overall, it does not successfully 
  predict when people will be detained.  Therefore, this model will not help
  Mexican authorities understand drivers of detention rates, since it does not
  consistently predict how the number of organized crime members wounded and weapons seized
  relate to whether people are detained or not. Furhermore, the model incorrectly
  predicted that people would be detained in 737 events when no one was detained.
  It also incorrectly predicted that people would not be detained when people were
  detained in 36 events.

__1c)__	be explicit about the limitations of your anaylisis, due to estimation 
or to the data itself
  
  * The limitations of the cubic regression model are shown through various diagnostic 
  plots using the entire dataset.  Refer to the analysis below the plots. 

```{r}
plot(lm(detained_resp ~ poly(weapons_seized + organized.crime.wounded, degree = 3),
        data = AllData)) # cubic regression

library(MASS)
hist(studres(lm(detained_resp ~ poly(weapons_seized + organized.crime.wounded, degree = 3),
                data = AllData)),
     main = "Distribution of Studentized Residuals") ## The Studentized 
## residuals, like standardized residuals, are normalized to unit 
## variance, but the Studentized version is fitted ignoring the current 
## data point. 
```  

  * __Residuals vs Fitted plot:__ This plot shows whether the residuals have non-linear 
  patterns.  Although the cubic model had the highest prediction accuracy of all
  the regression models,the residuals do not show a cubic relationship on this plot,
  but rather two linear lines that have a clear turning point.  Therefore, the cubic
  nature of the model may not actually be the most appropriate fit for the data. This
  may help explain why there was unbalanced prediction accuracy in that more "not detained"
  cases were predicted than "detained" cases. 
  
  * __Normal Q-Q plot:__ This plot shows whether the residuals are normally distributed.  
  Since the residuals on the graph do not follow the straight dashed line, the data are not 
  normally distributed.  This may represent bias or skewness in the data, which may be 
  attributed to unbalanced cases of detained versus not detained.  If this is the case, 
  this may support the claim that there are more cases of people not being detained.
  To understand the distribution of the data, the histrogram of the 
  `Distribution of Studentized Residuals` shows that the data are heavily skewed
  towards events when no one was detained.
  
  * __Scale-Location plot:__ This plot shows whether the residuals are spread 
  equally along the ranges of predictors.  Since the predictors are not equally 
  spread throughout the graph, this suggests that the data suffer from heterscedasticity, 
  meaning that variance among the residuals is not equal.  This seriously 
  undermines the validity of the model because the modeling errors may not be 
  uncorrelated and uniform. 
  
  * __Residuals vs Leverage plot:__ This plot illumiates outliers that may 
  be influential to regresson results.  The extreme outliers # 135 and 1429 
  are outside of the Cookâ€™s distance (represented by the dotted line).  These
  cases are influential to the regression results, which would be altered if
  these cases are excluded from the model.

  * __Conclusion:__ Given the analysis of the diagnostic plots above, the cubic 
  model has serious limitations that violate the basic regression assumptions.
  These limitations may invalidate the regression results.  The severity  of
  these limitations may become less influential as more data are collected.
  As more data are collected, theoretically, the distribution should normalize
  and become homoscedastic.
  
__1d)__ did you find something interesting? what is that? does your finding 
suggest this question is worth pursuing further? why or why not?
   
  * It is interesting that the cubic model predicts the best out of all the models
  but the predicitons are skewed towards predicting when no one will be detained.
  This skewness in the prediction may be attributed to the findings noted in #1c 
  above related to the violations of regression assumptions, especially that the 
  data are not normally distributed. Predicting whether or not people will be detained 
  using all weapons seized and wounded organized crime members as predictors is not
  worth pursuing because the model poorly predicts cases when people will be detained.
  Moreover, since the model is not in compliance with the regression assumptions, the
  results may not be true findings. 

__1e)__	if you did not find something interesting, explain why, and whether 
there is some additional information that would help in answering your question
 
  * The poor prediction accuracy of the cubic model in predicting when people will 
  be detained may be attributed to the skewness of the data.  Below, we see that 
  there were 3,787 events when no one was detained and 1,609 when people were 
  detained.  Therefore, the model may not have enough data to better predict when
  people will be detained.  The model may be more effective once more data are 
  collected and the data become normally distributed. 
  
```{r}
nrow(AllData[AllData$detained_resp == "0",]) # Number of events when no one was detained
nrow(AllData[AllData$detained_resp == "1",]) # Number of events when people were detained

data_dist <- matrix(c("3787",
                      "1609",
                      "No One Detained",
                      "Detained"),
                    ncol = 2,
                    nrow = 2)

data_dist <- as.data.frame(data_dist)

library(plyr)
data_dist <- rename(data_dist, c("V1" = "totals", "V2" = "type"))

library(ggplot2)

ggplot(data = data_dist, aes(x = totals, y = type, fill = totals)) +
  geom_bar(stat = "identity") +
  ggtitle("Distribution of Events When People Were Detained or Not") 


```

__1f)__	provide your code, and a single visualization per question that summarizes 
your finding

  * To contextualize the usefulness of the prediction model, the following 
  visualizations show the total number of weapons seized when people are detained 
  versus not detained and the total number of organized crime members wounded when 
  people are detained versus not detained.  These graphs should help the Mexican 
  government decide whether it is worth pursuing a strategy centered around detaining 
  more people if the goal is to maximize the number of weapons seized and organized crime
  members wounded.
  
```{r}
detained_subset <- subset(AllData,
                          detained_resp == 1,
                          select = c(weapons_seized, organized.crime.wounded))

not_detained_subset <- subset(AllData,
                              detained_resp == 0,
                              select = c(weapons_seized, organized.crime.wounded))

det_weap_total <- sum(detained_subset$weapons_seized) # total weapons seized 
# when people were detained

n_det_weap_total <- sum(not_detained_subset$weapons_seized) # total weapons seized 
# when people were NOT detained
 
det_ocw_total <- sum(detained_subset$organized.crime.wounded) # total number of
# organized crime members wounded when people were detained

n_det_ocw_total <- sum(not_detained_subset$organized.crime.wounded) # total number of
# organized crime members wounded when people were NOT detained

total_figures <- data.frame(det_weap_total,
                            n_det_weap_total,
                            det_ocw_total,
                            n_det_ocw_total) # convert to data.frame

library(plyr)
# rename column headings: 
total_figures <- rename(total_figures, 
                        c("det_weap_total" = "Weapons Seized (People Detained)", 
                          "n_det_weap_total" = "Weapons Seized (No One Detained)",
                          "det_ocw_total" = 
                            "Organized Crime Members Wounded (People Detained)",
                          "n_det_ocw_total" = 
                            "Organized Crime Members Wounded (No One Detained)")) 

# Have numbers appear with commas:
total_figures$`Weapons Seized (People Detained)` <- prettyNum(
  total_figures$`Weapons Seized (People Detained)`, 
  big.mark = ",",
  scientific = FALSE)

total_figures$`Weapons Seized (No One Detained)` <- prettyNum(
  total_figures$`Weapons Seized (No One Detained)`, 
  big.mark = ",",
  scientific = FALSE)

total_figures$`Organized Crime Members Wounded (People Detained)` <- prettyNum(
  total_figures$`Organized Crime Members Wounded (People Detained)`, 
  big.mark = ",",
  scientific = FALSE)

total_figures$`Organized Crime Members Wounded (No One Detained)` <- prettyNum(
  total_figures$`Organized Crime Members Wounded (No One Detained)`, 
  big.mark = ",",
  scientific = FALSE)

total_figures_new <- t(total_figures) # transpose total_figures for graphing purposes
total_figures_new <- as.data.frame(total_figures_new) # convert to data.frame
total_figures_new$subject <- row.names(total_figures_new) 
total_figures_new <- rename(total_figures_new,
                            c("V1" = "Totals"))

ggplot(data = total_figures_new, aes(y = Totals, x = subject, fill = subject)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("red", "green", "red", "green")) +
  coord_flip() +
  ggtitle("Total Weapons Seized and Organized Crime\nMembers Wounded Based on Detention") + # title line break
  geom_text(aes(label = Totals), vjust = 0, colour = "black") +
  theme(axis.line = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position = "none",
        panel.background = element_blank(),
        panel.border = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.background = element_blank())
```

  * The graph of all the data shows that when people are detained, more weapons
  are seized and organized crime members wounded.  If the Mexican government's 
  goal is to maximize the number of weapons seized and organized crime members 
  wounded, they should pursue a strategy that focuses on detaining people in 
  each event.  Although the cubic model does not predict when people will 
  be detained based on thee varaibles as well as it predicts when people will 
  not be detained, this issue may be solved once more data are collected. 
  The current dataset is heavily skewed towards cases when no one was detained, 
  as seen above in quest 1e.  Once more data are colelcted, the distribution should
  normlaize and the cubc model may have a more balanced success rate. 
  
__1g)__	phrase your finding for each question in two ways:

__1g-1)__	one sentence that summarizes your insight

  * A cubic model used the number of organized crime members wounded and weapons 
  seized per event to successfully predict when people will not be detained but 
  not when people will be detained with the same level of accuracy. 

__1g-2)__	one paragraph that reflects all nuance in your insight
 
  * A cubic model used the number of organized crime members wounded and weapons 
  seized per event to successfully predict when people will not be detained but 
  not when people will be detained with the same level of accuracy. The distribution
  of the data is not normal and skewed with more cases when people were not detained.
  Additionally, the data suffers from heteroscedasticity and extreme outliers that
  influence the regression results.  Overall, the cubic model violates multiple
  regression assumptions and the ouput is not useful in predicting when people will
  be detained.
        
__1h)__	make sure to also include your code
  
  * The code is included under each quetsion above.
  
#__Model #2 for question #1:__

__1.)__	Ask two (2) questions that might help you understand better the dynamics of 
violence contained on our datas et. Apply one algorithm per question and share 
your insights from each analysis. [50 pts] Remember: a non-finding is also a 
finding! It tells you whether a question is worth pursuing further or not.
  
  * Does the specific government group involved with an event (AFI, Army, 
  Federal Police, Ministerial Police, Municipal Police, Navy, Other, and State 
  Police) have a positive or negative relationship with Perfect Lethality? We can 
  see from the graph below that there are about 1500 cases with perfect lethality 
  therefore I believe it is acceptable to examine this question.
   
```{r}
library(ggplot2)
pl <- ggplot(AllData, aes(perfect.lethality))
pl + geom_bar()
```

__1a)__	perform the necessary transformations in your data - if any are needed, 
and explain why you did that

  * We used the same code used above to create a variable for total weaponry 
  seized (`weapons_seized`) by using the sum of the variables `clips.seized`,
  `cartridge.sezied`, `small.arms.seized`, `long.guns.seized`. This variable 
  will be used in order to control for all weaponary seized in our analysis.
  
```{r}
# used Stephanie's code to create one variable to sum weaponry seized
AllData$weapons_seized <- transform(AllData$clips.seized + 
                                      AllData$cartridge.sezied +
                                      AllData$small.arms.seized +
                                      AllData$long.guns.seized)
AllData$weapons_seized <- as.numeric(unlist((AllData$weapons_seized)))
```

__1b)__	show the output from your analysis in a consumable form
  
  * In order to answer our hypothesis we ran a multiple logistic regression on 
  perfect lethality by the involvment of the AFI, Army, Federal Police, 
  Ministerial Police, Municipal Police, Navy, Other, and State Police. A number 
  of control variables were also included which are municipality code, number of
  detained in the events, the date of each event, the total people dead in the 
  events, the total people wounded in the events, weaponry seized in the events, 
  and the source of the data for the events (Confrontations or Aggresions database). 
  We are using logistic regression becuase the variables being used are binary (0, 1).  

```{r}
logit.perf_leth3 <- glm(perfect.lethality ~ afi + army + federal.police + ministerial.police + municipal.police + navy + other + state.police + mun_code + detained + date + total.people.dead + total.people.wounded + weapons_seized + source, family = binomial (link = logit), data = AllData)
summary(logit.perf_leth3)
```

__1c__	be explicit about the limitations of your anaylisis, due to estimation 
or to the data itself

  * There are a number of limitations with the anaylisis. One limitation is 
  that not all government groups were involved in a large sum of events. For 
  example, the Navy was only involved in 89 cases and the AFI was only involved 
  in 13 cases. The graphs below display the cases in which the navy and AFI were 
  involved. Having such a small sample can be problematic.   

```{r}
afi <- ggplot(AllData, aes(afi))
afi + geom_bar() +
      ggtitle("AFI")

n <- ggplot(AllData, aes(navy))
n + geom_bar() +
    ggtitle("NAVY")
```

  * It should also be noted that the 'source' of the data was highly significant 
  in the model and if that variable is not included Ministerial Police, Municipal 
  Police, and State Police involvment all  become significant. I beleive this is 
  problematic becuase the two datasets might not be collected in the same fashion. 
  This model with and without the variable 'source' can be viewed below. It is also 
  a limitation that logistic models are more difficult to explain/interpret in 
  comparison to simple linear regression models. 
  
```{r}
logit.perf_leth2 <- glm(perfect.lethality ~ afi + army + federal.police + 
                          ministerial.police + municipal.police + navy + 
                          other + state.police + mun_code + detained + 
                          date + total.people.dead + total.people.wounded + 
                          weapons_seized, family = binomial (link = logit), 
                        data = AllData)
summary(logit.perf_leth2)
summary(logit.perf_leth3)
# explain limitation with datasets and also explain how logit are hard to explain/interpret
```

__1d)__	did you find something interesting? what is that? does your finding 
suggest this question is worth pursuing further? why or why not?

  * We found that the navy and the army seem to a statistically significant 
  and positive relationship to perfect lethality, controlling for all other 
  variables included in the model. This means that if the Army or Navy is 
  involved in an event the event has higher odds of being an event of perfect 
  lethality. I feel this finding does suggest this question is worth pursuing 
  further because it is important to know the effectivness of each individual 
  group to know what groups to send into specific situations. 
  
__1e)__	if you did not find something interesting, explain why, and whether 
there is some additional information that would help in answering your question

  * Not applicable. 
  
__1f)__	provide your code, and a single visualization per question that summarizes 
your finding
  
```{r fig.width=12, fig.height=12}
library(popbio)
par(mfrow = c(4, 2))

logi.hist.plot(AllData$army, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel = "ARMY Perfect Lethality", 
               xlabel = "ARMY Participation")

logi.hist.plot(AllData$afi, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel = "AFI Perfect Lethality", 
               xlabel = "AFI Participation")

logi.hist.plot(AllData$federal.police, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col="gray", 
               counts = TRUE, 
               mainlabel = "Federal Police Perfect Lethality", 
               xlabel = "Federal Police Participation")

logi.hist.plot(AllData$ministerial.police, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel = "Ministerial Police Perfect Lethality", 
               xlabel = "Ministerial Police Participation")

logi.hist.plot(AllData$municipal.police, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel = "Municipal Police Perfect Lethality", 
               xlabel = "Municipal Police Participation")

logi.hist.plot(AllData$navy, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE, 
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel = "NAVY Perfect Lethality", 
               xlabel = "NAVY Participation")

logi.hist.plot(AllData$other, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel ="OtherPerfect Lethality" , 
               xlabel = "Other Participation")

logi.hist.plot(AllData$state.police, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel = "State Police Perfect Lethality", 
               xlabel = "State Police Participation")
```


__1g)__	phrase your finding for each question in two ways:

__1g-1)__	one sentence that summarizes your insight
  
  * The invovlment of the Navy or Army in an event on average increase the 
  chances of said event being one of perfect lethality.

__1g-2)__	one paragraph that reflects all nuance in your insight
  
  * It is very interesting that the conttrol variables municipality code, number 
  of  detained in the events, the date of each event, the total people dead in 
  the events, the total people wounded in the events, and the source of the data 
  for the events (Confrontations or Aggresions database) all were statistically 
  significant. This means that the municipality in which the event took place is 
  related to whether or not the event was one of perfect lethality. This is a 
  question that could be further examined. It is also worth noting that the navy 
  was only included in 89 events yet 75 of the events were ones of perfect 
  lethality. When seeing those numbers it does seem clear that the navy is 
  important when it come to perfect lethality, yet they are not involved in 
  many cases whatsover. The Army on the other hand was involved in 1057 events
  and 797 of said events had perfect lethality. The army and the Navy seem to be 
  well trained at killing in comparison to the AFI, Federal Police, Ministerial 
  Police, Municipal Police, State Police, and other. 
        
__1h)__	make sure to also include your code
  
  * The code is included under each quetsion above.
  
#__Model #1 for question #2:__
  
__2.)__	Formulate two (2) conditional hypotheses that you seek to investigate with 
the data. One of your hypotheses should condition on two variables (as the 
example on the slides), and the other should condition on three variables. [50 pts]
    
  * ZACH 

```{r}

```

__2a)__	formulate each one of your hypotheses explicitly in substantive terms (as 
opposed to statistical terms) using 2-3 lines at most

  * 
  
__2b)__	show exactly how each one of your hypotheses translates into the marginal 
effect that you will seek to estimate from the data

  * 
  
__2c)__	show the output from your analysis in a consumable form
 
  * 

```{r}

```

__2d)__	show all your computations to estimate the corresponding marginal effect 
and its standard error

  * 

```{r}

```

__2e)__	be explicit in your assumptions

  * 
  
__2f)__	be explicit in the limitations of your inferences

  * 
  
__2g)__	phrase your finding for each question in two ways:

__2g-1)__	one sentence that summarizes your insight
  
  * 

__2g-2)__	one paragraph that reflects all nuance in your insight
  
  * 
  
__2h)__	make sure to also include your code
  
  * The code is included under each quetsion above.

#__Model #2 for question #2:__

__2.)__	Formulate two (2) conditional hypotheses that you seek to investigate with 
the data. One of your hypotheses should condition on two variables (as the 
example on the slides), and the other should condition on three variables. [50 pts]
    
  * ZACH:  

```{r}

```
  
__2a)__	formulate each one of your hypotheses explicitly in substantive terms (as 
opposed to statistical terms) using 2-3 lines at most

  * 
  
__2b)__	show exactly how each one of your hypotheses translates into the marginal 
effect that you will seek to estimate from the data

  * 
  
__2c)__	show the output from your analysis in a consumable form
  
  * 

```{r}

```

__2d)__	show all your computations to estimate the corresponding marginal effect 
and its standard error

  * 

```{r}

```

__2e)__	be explicit in your assumptions
  
  * 
  
__2f)__	be explicit in the limitations of your inferences
  
  * 
  
__2g)__	phrase your finding for each question in two ways:

__2g-1)__	one sentence that summarizes your insight

  *
  
__2g-2)__	one paragraph that reflects all nuance in your insight
  
  * 
  
__2h)__	make sure to also include your code
  
  * The code is included under each quetsion above.
  