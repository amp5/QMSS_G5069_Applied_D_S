Title: Data Challenge #2

Authors: Brandon Wolff, Zachary Heinemann, and Stephanie Langeland

Due date: 3/22/2017

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

__Review/load the data: __

```{r}
rm(list = ls(all = TRUE))   # cleans everything in the workspace

library(readr)          # easier reading of flat files
library(caret)          # classification and regresssion training package

# ::::::: SOME USEFUL DEFINITIONS :::::::::::::::::::::::::::::::::::::::::::::  

# set the general path for the project at its root, specific files will define 
# their own branches individually
# NOTE that specifying your path will be different in Windows

path <- "/Users/StephanieLangeland/Desktop/Columbia/Applied Data Science/Git/QMSS_G5069_Applied_D_S/Data Challenges/datachallenge2" ## Stephanie's path

# path <- "C:\\Users\\Brandon\\Documents\\GitHub\\QMSS_G5069_Applied_D_S\\Data Challenges\\datachallenge2" ## Brandon's path

# path <- "/Users/zachheinemann/GitHub/QMSS_G5069_Applied_D_S/Data Challenges/datachallenge2" ## Zachary's Path

# define additional paths for files you will use. In each case, determine
# appropriate additions to the path

inFileName1   <- "data/processed/AllViolenceData_170216.csv"     # cleaned data on violence
outFileName1  <- "graphs/RF_VarImportance.pdf"       
outFileName2  <- "graphs/RF_MSE.pdf"  

# ::::::: APPLY INITIAL DEFINITIONS ::::::::::::::::::::::::::::::::::::::::::: 

# set your path to that defined above, and confirm it
setwd(path)   
getwd()

# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: 
# :::::::::::::::::::::: LOADS DATA :::::::::::::::::::::::::::::::::::::::::::  


# ::::::: LOADING RAW DATA
# note that read_csv() guesses column types, so that date is read as a date 
# very useful for plotting time series

AllData <- read_csv(inFileName1) 

# rough validations that data was correctly loaded
## names(AllData)
## nrow(AllData)
## summary(AllData)

## The authories involved in each event are noted in their own columns in the 
## dataset.  For example when `AllData$army == 1` the army was involved.
## Guide to authority lookup:
auth_lookup <- "data/external/LookupAuthorityNames.csv"
auth_lookup_data <- read.csv(auth_lookup)
```

This is a team assignment. Please create a file on your team GitHub repo where 
you answer the challenge, including links to your code, and graphs.

# Model #1 for question #1:

__1.)__	Ask two (2) questions that might help you understand better the dynamics of 
violence contained on our dataset. Apply one algorithm per question and share 
your insights from each analysis. [50 pts] Remember: a non-finding is also a 
finding! It tells you whether a question is worth pursuing further or not.
  
  * Which type of regression model (linear, quadratic, cubic, quartic) best 
  predicts whether or not people will be detained using 
  all weapons (defined as `clips.seized`, `cartridge.sezied`, `small.arms.seized`, and
  `long.guns.seized`) seized and wounded organized crime members as predictors?
  Will this model help the Mexican government formulate a strategy when the goal
  is to maximize the number of people detained? 

__1a)__	perform the necessary transformations in your data - if any are needed, 
and explain why you did that

  * The variables `clips.seized`, `cartridge.sezied`, `small.arms.seized`, and
  `long.guns.seized` will be summed into one variable named `weapons_seized`.
  This new variable will allow us to analyze the total number of weapons seized.
  We want to understand how seizing any type of weapon relates to 
  whether people are detained rather than the number of each type of weapon
  seized.  We are including `clips.seized` because clips are part of weapons.
  
  * A new variable will be created named `detained_resp` to record whether people 
  where detained (`1`) or not (`0`).  We want to predict the response of whether
  people will be detained or not rather than the number of people detained.
  
```{r}
AllData$weapons_seized <- transform(AllData$clips.seized + 
                                      AllData$cartridge.sezied +
                                      AllData$small.arms.seized +
                                      AllData$long.guns.seized)

typeof(AllData$weapons_seized)
AllData$weapons_seized <- as.numeric(unlist((AllData$weapons_seized))) # convert 
# from list to numeric for regression models

AllData$detained_resp <- ifelse(AllData$detained > 0, 1, 0) 

typeof(AllData)
AllData <- as.data.frame(AllData) # convert from list to data.frame
```

__1b)__	show the output from your analysis in a consumable form
  
  * __Regression Models:__
  
```{r}
set.seed(1234)  # for reproducibility 
train <- sample(nrow(AllData), 2698) # randomly sample half of the data
training <- AllData[train, ] # create training set
testing <- AllData[-train, ] # create testing set

linear <- lm(detained_resp ~ weapons_seized + organized.crime.wounded, 
             data = training) # linear regression 

quadratic <- lm(detained_resp ~ poly(weapons_seized + organized.crime.wounded, degree = 2),
                data = training) # quadratic regression
 
cubic <- lm(detained_resp ~ poly(weapons_seized + organized.crime.wounded, degree = 3),
            data = training) # cubic regression

quartic <- lm(detained_resp ~ poly(weapons_seized + organized.crime.wounded, degree = 4),
              data = training) # quartic regression

p_linear <- predict(linear, newdata = testing, type = "response") # predict the repsonse 
p_liner_table <- confusionMatrix(testing$detained_resp, 
                                 as.integer(p_linear > 0.5)) # confusion matrix
 
p_quadratic <- predict(quadratic, newdata = testing, type = "response")
p_quadratic_table <- confusionMatrix(testing$detained_resp, as.integer(p_quadratic > 0.5)) 

p_cubic <- predict(cubic, newdata = testing, type = "response")
p_cubic_table <- confusionMatrix(testing$detained_resp, as.integer(p_cubic > 0.5))

p_quartic <- predict(quartic, newdata = testing, type = "response")
p_quartic_table <- confusionMatrix(testing$detained_resp, as.integer(p_quartic > 0.5))
```
  
  * __Regression Results:__
  A confusion matrix was constructed for each of the models and ranked in 
  order from worst to best prediction accuracy:
  
```{r}
p_quadratic_table # quadratic regression confusion matrix 
p_liner_table # linear regression confusion matrix 
p_quartic_table # quartic regression confusion matrix
p_cubic_table # cubic regression confusion matrix 
```
  
  * __Analysis of the output:__
  The cubic regression model had the highest 
  prediction accuracy of 0.7135 or 71.35% in using wounded organized crime members
  and all weapons seized to predict whether people would be detained or not.  The
  cubic model's confusion matrix shows that the model correctly predicted that no one
  would be detained 1,875 times and people would be detained 50 times. Although 
  the model has the highest prediction accuracy, it is mainly driven by correct 
  predictions of no one being detained.  Overall, it does not successfully 
  predict when people will be detained.  Therefore, this model will not help
  Mexican authorities understand drivers of detention rates, since it does not
  consistently predict how the number of organized crime members wounded and weapons seized
  relate to whether people are detained or not. Furhermore, the model incorrectly
  predicted that people would be detained in 737 events when no one was detained.
  It also incorrectly predicted that people would not be detained when people were
  detained in 36 events.

__1c)__	be explicit about the limitations of your anaylisis, due to estimation 
or to the data itself
  
  * The limitations of the cubic regression model are shown through various diagnostic 
  plots using the entire dataset.  Refer to the analysis below the plots. 

```{r}
plot(lm(detained_resp ~ poly(weapons_seized + organized.crime.wounded, degree = 3),
        data = AllData)) # cubic regression

library(MASS)
hist(studres(lm(detained_resp ~ poly(weapons_seized + organized.crime.wounded, degree = 3),
                data = AllData)),
     main = "Distribution of Studentized Residuals") ## The Studentized 
## residuals, like standardized residuals, are normalized to unit 
## variance, but the Studentized version is fitted ignoring the current 
## data point. 
```  

  * __Residuals vs Fitted plot:__ This plot shows whether the residuals have non-linear 
  patterns.  Although the cubic model had the highest prediction accuracy of all
  the regression models,the residuals do not show a cubic relationship on this plot,
  but rather two linear lines that have a clear turning point.  Therefore, the cubic
  nature of the model may not actually be the most appropriate fit for the data. This
  may help explain why there was unbalanced prediction accuracy in that more "not detained"
  cases were predicted than "detained" cases. 
  
  * __Normal Q-Q plot:__ This plot shows whether the residuals are normally distributed.  
  Since the residuals on the graph do not follow the straight dashed line, the data are not 
  normally distributed.  This may represent bias or skewness in the data, which may be 
  attributed to unbalanced cases of detained versus not detained.  If this is the case, 
  this may support the claim that there are more cases of people not being detained.
  To understand the distribution of the data, the histrogram of the 
  `Distribution of Studentized Residuals` shows that the data are heavily skewed
  towards events when no one was detained.
  
  * __Scale-Location plot:__ This plot shows whether the residuals are spread 
  equally along the ranges of predictors.  Since the predictors are not equally 
  spread throughout the graph, this suggests that the data suffer from heterscedasticity, 
  meaning that variance among the residuals is not equal.  This seriously 
  undermines the validity of the model because the modeling errors may not be 
  uncorrelated and uniform. 
  
  * __Residuals vs Leverage plot:__ This plot illumiates outliers that may 
  be influential to regresson results.  The extreme outliers # 135 and 1429 
  are outside of the Cook’s distance (represented by the dotted line).  These
  cases are influential to the regression results, which would be altered if
  these cases are excluded from the model.

  * __Conclusion:__ Given the analysis of the diagnostic plots above, the cubic 
  model has serious limitations that violate the basic regression assumptions.
  These limitations may invalidate the regression results.  The severity  of
  these limitations may become less influential as more data are collected.
  As more data are collected, theoretically, the distribution should normalize
  and become homoscedastic.
  
__1d)__ did you find something interesting? what is that? does your finding 
suggest this question is worth pursuing further? why or why not?
   
  * It is interesting that the cubic model predicts the best out of all the models
  but the predicitons are skewed towards predicting when no one will be detained.
  This skewness in the prediction may be attributed to the findings noted in #1c 
  above related to the violations of regression assumptions, especially that the 
  data are not normally distributed. Predicting whether or not people will be detained 
  using all weapons seized and wounded organized crime members as predictors is not
  worth pursuing because the model poorly predicts cases when people will be detained.
  Moreover, since the model is not in compliance with the regression assumptions, the
  results may not be true findings. 

__1e)__	if you did not find something interesting, explain why, and whether 
there is some additional information that would help in answering your question
  
```{r}
nrow(AllData[AllData$detained_resp == "0",]) # Number of events when no one was detained
nrow(AllData[AllData$detained_resp == "1",]) # Number of events when people were detained

data_dist <- matrix(c("3787",
                      "1609",
                      "No One Detained",
                      "Detained"),
                    ncol = 2,
                    nrow = 2)

data_dist <- as.data.frame(data_dist)

library(plyr)
data_dist <- rename(data_dist, c("V1" = "totals", "V2" = "type"))

library(ggplot2)

ggplot(data = data_dist, aes(x = totals, y = type, fill = totals)) +
  geom_bar(stat = "identity") +
  ggtitle("Distribution of Events When People Were Detained or Not") +
  scale_fill_discrete(name = "Legend",
                      breaks = c("1609", "3787"),
                      labels = c("People Detained", "No One Detained")) +
  geom_text(aes(label = totals, vjust = 0)) +
  theme(
    axis.text.x = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )
```


  * The poor prediction accuracy of the cubic model in predicting when people will 
  be detained may be attributed to the skewness of the data.  Above, we see that 
  there were 3,787 events when no one was detained and 1,609 when people were 
  detained.  Therefore, the model may not have enough data to better predict when
  people will be detained.  The model may be more effective once more data are 
  collected and the data become normally distributed. 
  
__1f)__	provide your code, and a single visualization per question that summarizes 
your finding

  * To contextualize the usefulness of the prediction model, the following 
  visualization shows the total number of weapons seized when people are detained 
  versus not detained and the total number of organized crime members wounded when 
  people are detained versus not detained.  This graph should help the Mexican 
  government decide whether it is worth pursuing a strategy centered around detaining 
  more people if the goal is to maximize the number of weapons seized and organized crime
  members wounded.
  
```{r}
# Create a smaller dataset to build the visualization: 

# Subset of `weapons_seized` and `organized.crime.wounded` when peoeple were detained:
detained_subset <- subset(AllData,
                          detained_resp == 1,
                          select = c(weapons_seized, organized.crime.wounded))

# Subset of `weapons_seized` and `organized.crime.wounded` when no one was detained:
not_detained_subset <- subset(AllData,
                              detained_resp == 0,
                              select = c(weapons_seized, organized.crime.wounded))

det_weap_total <- sum(detained_subset$weapons_seized) # total weapons seized 
# when people were detained

n_det_weap_total <- sum(not_detained_subset$weapons_seized) # total weapons seized 
# when people were NOT detained
 
det_ocw_total <- sum(detained_subset$organized.crime.wounded) # total number of
# organized crime members wounded when people were detained

n_det_ocw_total <- sum(not_detained_subset$organized.crime.wounded) # total number of
# organized crime members wounded when people were NOT detained

total_figures <- data.frame(det_weap_total,
                            n_det_weap_total,
                            det_ocw_total,
                            n_det_ocw_total) # convert to data.frame

library(plyr)
# rename column headings: 
total_figures <- rename(total_figures, 
                        c("det_weap_total" = "Weapons Seized (People Detained)", 
                          "n_det_weap_total" = "Weapons Seized (No One Detained)",
                          "det_ocw_total" = 
                            "Organized Crime Members Wounded (People Detained)",
                          "n_det_ocw_total" = 
                            "Organized Crime Members Wounded (No One Detained)")) 

# Have numbers appear with commas:
total_figures$`Weapons Seized (People Detained)` <- prettyNum(
  total_figures$`Weapons Seized (People Detained)`, 
  big.mark = ",",
  scientific = FALSE)

total_figures$`Weapons Seized (No One Detained)` <- prettyNum(
  total_figures$`Weapons Seized (No One Detained)`, 
  big.mark = ",",
  scientific = FALSE)

total_figures$`Organized Crime Members Wounded (People Detained)` <- prettyNum(
  total_figures$`Organized Crime Members Wounded (People Detained)`, 
  big.mark = ",",
  scientific = FALSE)

total_figures$`Organized Crime Members Wounded (No One Detained)` <- prettyNum(
  total_figures$`Organized Crime Members Wounded (No One Detained)`, 
  big.mark = ",",
  scientific = FALSE)

total_figures_new <- t(total_figures) # transpose total_figures for graphing purposes
total_figures_new <- as.data.frame(total_figures_new) # convert to data.frame
total_figures_new$subject <- row.names(total_figures_new) 
total_figures_new <- rename(total_figures_new,
                            c("V1" = "Totals"))

ggplot(data = total_figures_new, aes(y = Totals, x = subject, fill = subject)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("red", "green", "red", "green")) +
  coord_flip() +
  ggtitle("Total Weapons Seized and Organized Crime\nMembers Wounded Based on Detention") + # title line break
  geom_text(aes(label = Totals), vjust = 0, colour = "black") +
  theme(axis.line = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position = "none",
        panel.background = element_blank(),
        panel.border = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.background = element_blank())
```

  * The graph of all the data shows that when people are detained, more weapons
  are seized and organized crime members wounded.  If the Mexican government's 
  goal is to maximize the number of weapons seized and organized crime members 
  wounded, they should pursue a strategy that focuses on detaining people in 
  each event.  Although the cubic model does not predict when people will 
  be detained based on thee varaibles as well as it predicts when people will 
  not be detained, this issue may be solved once more data are collected. 
  The current dataset is heavily skewed towards cases when no one was detained, 
  as seen above in question ___1e___.  Once more data are colelcted, the distribution should
  normalize and the cubic model may have a more balanced success rate. 
  
__1g)__	phrase your finding for each question in two ways:

__1g-1)__	one sentence that summarizes your insight

  * A cubic model used the number of organized crime members wounded and weapons 
  seized per event to successfully predict when people will not be detained but 
  not when people will be detained with the same level of accuracy. 

__1g-2)__	one paragraph that reflects all nuance in your insight
 
  * A cubic model used the number of organized crime members wounded and weapons 
  seized per event to successfully predict when people will not be detained but 
  not when people will be detained with the same level of accuracy. The distribution
  of the data is not normal and skewed towards more cases when people were not detained.
  Additionally, the data suffers from heteroscedasticity and extreme outliers that
  influence the regression results.  Overall, the cubic model violates many
  regression assumptions and the ouput is not useful in predicting when people will
  be detained.  
        
__1h)__	make sure to also include your code
  
  * The code is included under each quetsion above.
  
#__Model #2 for question #1:__

__1.)__	Ask two (2) questions that might help you understand better the dynamics of 
violence contained on our datas et. Apply one algorithm per question and share 
your insights from each analysis. [50 pts] Remember: a non-finding is also a 
finding! It tells you whether a question is worth pursuing further or not.
  
  * Does the specific government group involved with an event (AFI, Army, 
  Federal Police, Ministerial Police, Municipal Police, Navy, Other, and State 
  Police) have a positive or negative relationship with Perfect Lethality? We can 
  see from the graph below that there are about 1500 cases with perfect lethality 
  therefore I believe it is acceptable to examine this question.
   
```{r}
library(ggplot2)
pl <- ggplot(AllData, aes(perfect.lethality))
pl + geom_bar()
```

__1a)__	perform the necessary transformations in your data - if any are needed, 
and explain why you did that

  * We used the same code used above to create a variable for total weaponry 
  seized (`weapons_seized`) by using the sum of the variables `clips.seized`,
  `cartridge.sezied`, `small.arms.seized`, `long.guns.seized`. This variable 
  will be used in order to control for all weaponary seized in our analysis.
  
```{r}
# used Stephanie's code to create one variable to sum weaponry seized
AllData$weapons_seized <- transform(AllData$clips.seized + 
                                      AllData$cartridge.sezied +
                                      AllData$small.arms.seized +
                                      AllData$long.guns.seized)
AllData$weapons_seized <- as.numeric(unlist((AllData$weapons_seized)))
```

__1b)__	show the output from your analysis in a consumable form
  
  * In order to answer our hypothesis we ran a multiple logistic regression on 
  perfect lethality by the involvment of the AFI, Army, Federal Police, 
  Ministerial Police, Municipal Police, Navy, Other, and State Police. A number 
  of control variables were also included which are municipality code, number of
  detained in the events, the date of each event, the total people dead in the 
  events, the total people wounded in the events, weaponry seized in the events, 
  and the source of the data for the events (Confrontations or Aggresions database). 
  We are using logistic regression becuase the variables being used are binary (0, 1).  

```{r}
logit.perf_leth3 <- glm(perfect.lethality ~ afi + army + federal.police + ministerial.police + municipal.police + navy + other + state.police + mun_code + detained + date + total.people.dead + total.people.wounded + weapons_seized + source, family = binomial (link = logit), data = AllData)
summary(logit.perf_leth3)
```

__1c__	be explicit about the limitations of your anaylisis, due to estimation 
or to the data itself

  * There are a number of limitations with the anaylisis. One limitation is 
  that not all government groups were involved in a large sum of events. For 
  example, the Navy was only involved in 89 cases and the AFI was only involved 
  in 13 cases. The graphs below display the cases in which the navy and AFI were 
  involved. Having such a small sample can be problematic.   

```{r}
afi <- ggplot(AllData, aes(afi))
afi + geom_bar() +
      ggtitle("AFI")

n <- ggplot(AllData, aes(navy))
n + geom_bar() +
    ggtitle("NAVY")
```

  * It should also be noted that the 'source' of the data was highly significant 
  in the model and if that variable is not included Ministerial Police, Municipal 
  Police, and State Police involvment all  become significant. I beleive this is 
  problematic becuase the two datasets might not be collected in the same fashion. 
  This model with and without the variable 'source' can be viewed below. It is also 
  a limitation that logistic models are more difficult to explain/interpret in 
  comparison to simple linear regression models. 
  
```{r}
logit.perf_leth2 <- glm(perfect.lethality ~ afi + army + federal.police + 
                          ministerial.police + municipal.police + navy + 
                          other + state.police + mun_code + detained + 
                          date + total.people.dead + total.people.wounded + 
                          weapons_seized, family = binomial (link = logit), 
                        data = AllData)
summary(logit.perf_leth2)
summary(logit.perf_leth3)
# explain limitation with datasets and also explain how logit are hard to explain/interpret
```

__1d)__	did you find something interesting? what is that? does your finding 
suggest this question is worth pursuing further? why or why not?

  * We found that the navy and the army seem to a statistically significant 
  and positive relationship to perfect lethality, controlling for all other 
  variables included in the model. This means that if the Army or Navy is 
  involved in an event the event has higher odds of being an event of perfect 
  lethality. I feel this finding does suggest this question is worth pursuing 
  further because it is important to know the effectivness of each individual 
  group to know what groups to send into specific situations. 
  
__1e)__	if you did not find something interesting, explain why, and whether 
there is some additional information that would help in answering your question

  * Not applicable. 
  
__1f)__	provide your code, and a single visualization per question that summarizes 
your finding
  
```{r fig.width=12, fig.height=12}
library(popbio)
par(mfrow = c(4, 2))

logi.hist.plot(AllData$army, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel = "ARMY Perfect Lethality", 
               xlabel = "ARMY Participation")

logi.hist.plot(AllData$afi, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel = "AFI Perfect Lethality", 
               xlabel = "AFI Participation")

logi.hist.plot(AllData$federal.police, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col="gray", 
               counts = TRUE, 
               mainlabel = "Federal Police Perfect Lethality", 
               xlabel = "Federal Police Participation")

logi.hist.plot(AllData$ministerial.police, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel = "Ministerial Police Perfect Lethality", 
               xlabel = "Ministerial Police Participation")

logi.hist.plot(AllData$municipal.police, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel = "Municipal Police Perfect Lethality", 
               xlabel = "Municipal Police Participation")

logi.hist.plot(AllData$navy, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE, 
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel = "NAVY Perfect Lethality", 
               xlabel = "NAVY Participation")

logi.hist.plot(AllData$other, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel ="OtherPerfect Lethality" , 
               xlabel = "Other Participation")

logi.hist.plot(AllData$state.police, 
               AllData$perfect.lethality, 
               boxp = FALSE, 
               rug = FALSE,
               logi.mod = 1, 
               type = "hist", 
               col = "gray", 
               counts = TRUE, 
               mainlabel = "State Police Perfect Lethality", 
               xlabel = "State Police Participation")
```


__1g)__	phrase your finding for each question in two ways:

__1g-1)__	one sentence that summarizes your insight
  
  * The invovlment of the Navy or Army in an event on average increase the 
  chances of said event being one of perfect lethality.

__1g-2)__	one paragraph that reflects all nuance in your insight
  
  * It is very interesting that the conttrol variables municipality code, number 
  of  detained in the events, the date of each event, the total people dead in 
  the events, the total people wounded in the events, and the source of the data 
  for the events (Confrontations or Aggresions database) all were statistically 
  significant. This means that the municipality in which the event took place is 
  related to whether or not the event was one of perfect lethality. This is a 
  question that could be further examined. It is also worth noting that the navy 
  was only included in 89 events yet 75 of the events were ones of perfect 
  lethality. When seeing those numbers it does seem clear that the navy is 
  important when it come to perfect lethality, yet they are not involved in 
  many cases whatsover. The Army on the other hand was involved in 1057 events
  and 797 of said events had perfect lethality. The army and the Navy seem to be 
  well trained at killing in comparison to the AFI, Federal Police, Ministerial 
  Police, Municipal Police, State Police, and other. 
        
__1h)__	make sure to also include your code
  
  * The code is included under each quetsion above.
  
#__Model #1 for question #2:__
  
__2.)__	Formulate two (2) conditional hypotheses that you seek to investigate with 
the data. One of your hypotheses should condition on two variables (as the 
example on the slides), and the other should condition on three variables. [50 pts]
    
  * ZACH 

```{r}
fit <- lm(total.people.dead ~ cartridge.sezied + army + federal.police + ministerial.police + municipal.police + navy + state.police + long.guns.seized * source + small.arms.seized + clips.seized + vehicles.seized, data = AllData)
```

__2a)__	formulate each one of your hypotheses explicitly in substantive terms (as 
opposed to statistical terms) using 2-3 lines at most

  * Our first hypothesis is such that there is a relationship between long guns seized and number of people dead such that as more long guns are seized (a proxy for severity of the incident as long guns are most lethal of the arms listed), there will be more people dead; further, becuase of the clear differences in datasets (seen in exploratory analysis), we expect that this relationship may differ as a function of the source of the dataset.
  
__2b)__	show exactly how each one of your hypotheses translates into the marginal 
effect that you will seek to estimate from the data

  * Given that the Aggressions dataset appears to have less lethality, we may expect that the relationship between long guns seized and people dead would be lower in this dataset as opposed to the confrontations dataset -- therefore the interaction term would be positive and the marginal effect of 'source' being the confrontations (as opposed to aggression) would be higher than if we were to estimate the relationship without this conditional variable. Rather, we expect the positive relationship between long guns seized and people dead to be lower for the aggressions data, therefore making the marginal effect an effect lower than the effect for 1) the confrontation dataset and 2) the overall dataset aggregating between confrontations and aggresions.
  
__2c)__	show the output from your analysis in a consumable form
 
```{r}
summary(fit)
```

  * This model includes a series of controls in order to adequately control for other possible mechanisms related to the outcome (see assumptions section for further discussion of this). For the purpose of the output, we will focus on the two constituent main effects, long guns seized and source, as well as their interaction. 
  * First, the long guns seized term indicates that for every one more long guns seized, .984 more people are dead net of all other factors and when source is the aggressions data, but this is not significant at the .05 level. This is the opposite of what we originally hypothesized. 
  * The source term indicates that on average in the confrontations dataset, net of all other factors and when long guns seized is 0, there are 7.106 more people dead than in the aggressions dataset and it is significant at the .001 level. As aforementioned, conditional hypotheses give new meanings to the constituent variables. Rather, the terms for 'source' and 'long guns seized' indicate net of all other factors and when the other variable is at zero. 
  
  * The interaction term is testing for a differences in slopes; specifically whether or not the relationship between long guns seized and total people dead is significantly different for the aggression versus the confrontations dataset. This term indicates that when the dataset is confrontations, every one unit increase in long guns seized (or for every additional long gun seized), there are 1.992 more dead than the original long gun seized term (which is .984). Substantively, this indicates that the slope between long guns seized and total people dead is 1.992 more for the confrontations dataset than the aggressions dataset. The interaction term is significant at the .05 level.

__2d)__	show all your computations to estimate the corresponding marginal effect 
and its standard error

  * The marginal effect, as indicated in class, can be found by compounding the original slope term (in our case, the long gun seized term) and the interaction term. Therefore, this would be .984 + 1.992 = 2.976. This indicates that when the dataset is confrontations (as opposed to aggressions), for every long gun seized, 2.976 more people are dead.
  * The standard error of the marginal effect is calculated using the following formula: var(B1) + Z^2 * var(B3) + 2ZCov(B1, B3), where in our case:
  B1: long guns seized slope
  Z: source (0 = aggressions, 1 = confrontations)
  B3: long guns seized * source interaction term
  
We can first extract the variance - covariance matrix necessary to calculate this using the following code:

```{r}
vcov(fit)
```

From this, we can calculate:
```{r}
var <- .002647219 + 1 * 1 *.002573099 + 2 * 1 * .002557878
var
sqrt(var)
```

Therefore the standard error of the marginal effect is .10167 (which matches the interaction term of the regression).

__2e)__	be explicit in your assumptions

  * There are quite a number of assumptions within this model. First off, a linear regression model was used -- which indicates that we are assuming the dependent variable to be continuous and that there is a linear relationship between the predictors and the dependent variable. The dependent variable is not properly continuous as it can only have integer values and it cannot have any negative values; however for the purpose of drawing overall insight we may still be able to use this method to gain some understanding of the relationship between the variables, but a poisson or zero-inflated binomial regression would theoretically be better suited for the nature of the data.
  * A linear model, by definition, assumes that the predictors are not significantly correlated to one another (rather, that they are orthogonal). In this case, it is likely that these variables are at least somewhat correlated as the presence of one type of armed force (i.e. army) is likely correlated with another, as is the type of weaponry seized is likely correlated as well (long guns, for example, require cartridges in order to operate, therefore these metrics are likely correlated). However, this assumption may be somewhat malleable depending on how correlated the variables are, and insight can still be discerned overall from the model -- although one must be skeptical of the insight gleaned.
  * Because the dependent variable is a composite, we are collapsing different forms of death. As this model does not condition on the depenvent variable, we are making the implicit assumption that the predictors predict different types of death (i.e. civilian vs. non-civilian) to be equivalent, as the two are collapsed into total and the model is predicting this aggregated value. This perhaps is not necessarily the case and one could, with more complicated machinery, condition on the dependent variable as well to account for the potentially differential relationship.
  * Furthermore, it is worth noting that because this model does not condition on any other variables or does not include an interaction between 'source' and any other variables, it is implicitly assuming that the relationship between any other predictor and total dead is constant as a function of source, which is likely not the case.
  
__2f)__	be explicit in the limitations of your inferences

  * The model has a series of limitations insofar as the dependent variable is not necessarily distributed ideally for the model used. Because this is not a count variable, one must take the interpretation of the model with skepticism (for example, some combination of predictors could result in non-integer or negative number of deaths, both of which are non-possible values).
  * Additionally, one must recognize that the inter-correlation between the predictors means that these estimates may be even more unstable than one might think and therefore we are limited in the extent to which we are confident in any of the model's parameters due to this inter-correlation.
  *It is worth noting that there is a censorship on the dependent variable, as only the cases in which they are defined as a confrontation are documented, and therefore this censorship means that the inferences that one can make from the data are limited by the collection and scope of the data itself, indicating that perhaps we should be skeptical of the extent to which the estimates and insights from this dataset may extrapolate out of the scope of time, location, and data gathering method that this data possesses.
  
__2g)__	phrase your finding for each question in two ways:

__2g-1)__	one sentence that summarizes your insight
  
  * The more long guns seized, the more people dead in the incidence and this relationship is even stronger for the data gathered in the confrontations dataset than the aggressions dataset.

__2g-2)__	one paragraph that reflects all nuance in your insight
  
  * There appears to be some evidence of a relationship that indicates the more long guns seized, the more dangerous (lethal) the situation is overall. Furthermore, the relationship between long gun seizure and death is three times stronger for the cases in the confrontations dataset than the aggressions dataset, controlling for all of the other types of ammunition seized and types of armed forces involved. The relationship between long guns and death is not statistically significant in the aggressions dataset, but it is in the confrontations set and the two relationships are significantly different -- indicating that these two datasets are clearly examining quite different cases and capturing quite different relationships, while claiming to be coming from the same data-generating process. This is case for further investigation of the data gathering processes of these two datasets and further examination of why lethality differs so greatly, as well as seizure lethality relationships, between two ostensibly identical data-generating processes.
  
__2h)__	make sure to also include your code
  
  * The code is included under each quetsion above.

#__Model #2 for question #2:__

__2.)__	Formulate two (2) conditional hypotheses that you seek to investigate with 
the data. One of your hypotheses should condition on two variables (as the 
example on the slides), and the other should condition on three variables. [50 pts]
    
  * ZACH:  

```{r}
fit <- lm(total.people.dead ~ cartridge.sezied + federal.police + ministerial.police + municipal.police + navy + state.police + long.guns.seized * source * army + small.arms.seized + clips.seized + vehicles.seized, data = AllData)
```
  
__2a)__	formulate each one of your hypotheses explicitly in substantive terms (as 
opposed to statistical terms) using 2-3 lines at most

  * As before, there is reason to believe that the relationship between seizure of long guns and death varies as a function of armed forces present; we hypothesize that the difference in this relationship previously found with different data sources is further delineated by whether or not the army is present -- with differences in the relationship being observable only in the confrontation set.
  
__2b)__	show exactly how each one of your hypotheses translates into the marginal 
effect that you will seek to estimate from the data

  * We will estimate whether or not the previous two-way interaction can be extended into a three-way interaction such that this differential relationship between data sets of the relationship between long-gun seizure and total deaths is stronger when the army is present only in the confrontation set and not necessarily the aggression set. Rather, the marginal effect will be if conditioning on presence of army adds additional information about the relationship between long-gun seizure and death that may differ between the two datasets.
  
__2c)__	show the output from your analysis in a consumable form

```{r}
summary(fit)
```

  * This model includes a series of controls in order to adequately control for other possible mechanisms related to the outcome (see assumptions section for further discussion of this). For the purpose of the output, we will focus on the three constituent main effects, long guns seized, source and army, as well as their two-way interactions, and then the three way interaction term (the main conditional hypothesis).
  * The first main effect is long guns seized; the term in the model indicates that when army and source are 0 (indicating an instance from the aggression set with no army presence), for every long gun seized, .045 more people are dead net of all other factors, but this is not significant at the .05 level.
  * The second main effect is source; the term in the model indicates that when long guns seized and army are 0 (indicating an instance with no long guns seized and no army presence), the confrontations set on average has .449 more deaths than the aggression set per instance net of all other factors and this is significant at the .001 level.
  * The third main effect is army; the term in the model indicates that when long guns seized and source are 0 ( indicating an instance from the aggression set with no long guns seized), having army presence indicates on average .458 less deaths net of all other factors and this is significant at the .05 level.
  * The first two-way interaction is between long guns seized and source such that the slope between long guns seized and deaths is .168 higher for the confrontations data set than the aggressions dataset net of all other factors and when army is not present; it is significant at the .01 level.
  * The second two-way interaction is between long guns seized and army such that the slope between long guns seized and deaths is .0056 lower when the army is present as opposed to when the army is present net of all other factors and when source is 0 (meaning in the aggressions dataset). This term, however, is not statistically significant at the .05 level.
  * The third two-way interaction is between source and army such that when looking at the confrontations dataset and the army is present, the average number of deaths is .1233 higher than when neither or only one of them is present net of all other factors and when long gun seizure is 0. This term, however, is not statistically significant at the .05 level.
  * The three-way interaction term (or three way conditional hypothesis) is testing whether the slope of long guns predicting death varies as a function of both source and army. The term indicates that when source and army are 1 (indicating the army is present and the source is the confrontations set), the effect of long guns seizes on total deaths is -.085 less when neither or only one is present net of all other factors. This term, however, is not statistically significant at the .05 level. See next section for further discussion for the marginal effect estimated by this term.
  
__2d)__	show all your computations to estimate the corresponding marginal effect 
and its standard error

  * The marginal effect for a three variable conditioning hypothesis will be formulated using tables from Aiken & West (1991; see attached picture for formulas) which give the marginal effects and standard errors for a three-way interaction. 
  
![Formulas for Marginal Effects](/Users/zachheinemann/Downloads/AikenWest.jpg)
  
  The formula for marginal effect is the long gun term added with the army -- long gun interaction term, the source -- long gun interaction term, and the three-way interaction term like such:
  
  .0452 + .1683 - .0056 - .0846 = .1233
  
  This indicates that when army is present and it is the confrontations dataset, each long gun seized results in .1233 more deaths. Let's now calculate the standard error of this marginal effect.
  
  * The standard error of the marginal effect is calculated using the following formula: var(B1) + Z^2 var(B4) + W^2 var(B5) + Z^2 W^2 var(B7) + 2ZCov(B1, B4) + 2Wcov(B1,B5) + 2ZWcov(B1,B7) + 2ZWcov(B4,B5) + 2WZ^2cov(B4, B7) + 2ZW^2cov(B5, B7), where in our case:
  B1: long guns seized slope
  Z: source (0 = aggressions, 1 = confrontations)
  W: army (0 = no army, 1 = army)
  B4: long guns seized * source interaction term
  B5: long guns seized * army interaction term
  B7: long guns seized, army, source interaction term
  
We can first extract the variance - covariance matrix necessary to calculate this using the following code:

```{r}
vcov(fit)
```

From this, we can calculate (because Z and W are 1, they are dropped from this equation for simplicity's sake):
```{r}
var <- 5.780256e-03 + 5.824379e-03 + 1.074442e-02 + 1.093731e-02 + 2 * -5.699216e-03 + 2 * -5.687154e-03 + 2 * 5.697308e-03 + 2 * 5.670264e-03 + 2 * -5.829394e-03 + 2 * -1.073217e-02
var
sqrt(var)
```

Becuase we are just evaluating the standard error of the marginal effect, it does not necessarily match the standard error of the three-way interaciton term itself (as this is also including the two-way interaction between source and army, which does not impact the marginal effect of long guns on total death in this instance), the standard error of the marginal effect is .0112.

__2e)__	be explicit in your assumptions
  
  * Because of similarities between this model and the last, some of the assumptions stated are repeated here.
  * There are quite a number of assumptions within this model. First off, a linear regression model was used -- which indicates that we are assuming the dependent variable to be continuous and that there is a linear relationship between the predictors and the dependent variable. The dependent variable is not properly continuous as it can only have integer values and it cannot have any negative values; however for the purpose of drawing overall insight we may still be able to use this method to gain some understanding of the relationship between the variables, but a poisson or zero-inflated binomial regression would theoretically be better suited for the nature of the data.
  * A linear model, by definition, assumes that the predictors are not significantly correlated to one another (rather, that they are orthogonal). In this case, it is likely that these variables are at least somewhat correlated as the presence of one type of armed force (i.e. army) is likely correlated with another, as is the type of weaponry seized is likely correlated as well (long guns, for example, require cartridges in order to operate, therefore these metrics are likely correlated). However, this assumption may be somewhat malleable depending on how correlated the variables are, and insight can still be discerned overall from the model -- although one must be skeptical of the insight gleaned.
  * Because the dependent variable is a composite, we are collapsing different forms of death. As this model does not condition on the depenvent variable, we are making the implicit assumption that the predictors predict different types of death (i.e. civilian vs. non-civilian) to be equivalent, as the two are collapsed into total and the model is predicting this aggregated value. This perhaps is not necessarily the case and one could, with more complicated machinery, condition on the dependent variable as well to account for the potentially differential relationship.
  * Furthermore, it is worth noting that because this model does not condition on any other variables or does not include an interaction between 'source' and any other variables, it is implicitly assuming that the relationship between any other predictor and total dead is constant as a function of source, which is likely not the case.
  * Additionally, this model is making an assumption or rather including a term such that the two-way relationship between long gun seizure and total deaths varies as a function of whether or not the army is present (which is what the three-way interaction term includes), but this assumption that these relationships vary as a function of whether or not army is present may not be true (and given the context of our model and the p-value, does not appear to hold true). Therefore, in allowing for this three-way interaction we are making an implicit assumption of allowing the relationships to vary as a function of the conditioning on the third variable.

  
__2f)__	be explicit in the limitations of your inferences
  
  * Because of similarities between this model and the last, some of the limitations stated are repeated here.
  * The model has a series of limitations insofar as the dependent variable is not necessarily distributed ideally for the model used. Because this is not a count variable, one must take the interpretation of the model with skepticism (for example, some combination of predictors could result in non-integer or negative number of deaths, both of which are non-possible values).
  * Additionally, one must recognize that the inter-correlation between the predictors means that these estimates may be even more unstable than one might think and therefore we are limited in the extent to which we are confident in any of the model's parameters due to this inter-correlation.
  *It is worth noting that there is a censorship on the dependent variable, as only the cases in which they are defined as a confrontation are documented, and therefore this censorship means that the inferences that one can make from the data are limited by the collection and scope of the data itself, indicating that perhaps we should be skeptical of the extent to which the estimates and insights from this dataset may extrapolate out of the scope of time, location, and data gathering method that this data possesses.
  * There is a limitation in the three-way interaction term because of the relative number of scenarios in each cell. For example, there is not an equivalent number of instances whether army = 1 and source = 0 as compared with when army = 0 and source = 1. The interaction term is allowing for there to be differential long gun seizure -- death count slopes amongst the four categories and a marginal effect when both are present. However, the different sample sizes in the various cells may limit the extent to which particular slopes may be estimated and whether or not a subsequent interaction term can be estimated; making the overall inferences from the interaction term limited in nature. A more deliberately balanced dataset may ameliorate this problem, although this then ignores the naturalistic relationship between these variables occurring together.
  
__2g)__	phrase your finding for each question in two ways:

__2g-1)__	one sentence that summarizes your insight

  * The more long guns seized, the more deaths overall in the confrontations data but not the aggressions data and this effect does not change whether the army is there or not regardless of dataset.
  
__2g-2)__	one paragraph that reflects all nuance in your insight
  
  * The more long guns seized, the more deaths overall in the confrontations data but not the aggressions data. However, there is no significant relationship between long guns and deaths in the aggressions dataset when army is present or army is not present. Furthermore, the presence of the army in the confrontations set does not significantly impact the relationship between long gun seizure and deaths above and beyond the previously observed relationship between the two in the confrontations dataset irrespective of the presence of the army.
  
__2h)__	make sure to also include your code
  
  * The code is included under each quetsion above.
  