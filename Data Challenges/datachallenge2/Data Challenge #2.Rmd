---
title: 'Data Challenge #2'
authors: "Brandon Wolff, Zachary Heinemann, and Stephanie Langeland"
due date: "3/22/2017"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Review the data:
```{r}
rm(list = ls(all = TRUE))   # cleans everything in the workspace

library(readr)          # easier reading of flat files
library(caret)          # classification and regresssion training package
library(randomForest)   # Random Forests package

# ::::::: SOME USEFUL DEFINITIONS :::::::::::::::::::::::::::::::::::::::::::::  

# set the general path for the project at its root, specific files will define 
# their own branches individually
# NOTE that specifying your path will be different in Windows

path <- "/Users/StephanieLangeland/Desktop/Columbia/Applied Data Science/Git/QMSS_G5069_Applied_D_S/Data Challenges/datachallenge2" ## Stephanie's path

# path <- "" ## Brandon's path

# path <- "" ## Zach's path

# define additional paths for files you will use. In each case, determine
# appropriate additions to the path

inFileName1   <- "data/processed/AllViolenceData_170216.csv"     # cleaned data on violence
outFileName1  <- "graphs/RF_VarImportance.pdf"       
outFileName2  <- "graphs/RF_MSE.pdf"  

# ::::::: APPLY INITIAL DEFINITIONS ::::::::::::::::::::::::::::::::::::::::::: 

# set your path to that defined above, and confirm it
setwd(path)   
getwd()

# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: 
# :::::::::::::::::::::: LOADS DATA :::::::::::::::::::::::::::::::::::::::::::  


# ::::::: LOADING RAW DATA
# note that read_csv() guesses column types, so that date is read as a date 
# very useful for plotting time series

AllData <- read_csv(inFileName1) 

# rough validations that data was correctly loaded
names(AllData)
nrow(AllData)
summary(AllData)

## The authories involved in each event are noted in their own columns in the 
## dataset.  For example when `AllData$army == 1` the army was involved.
## Guide to authority lookup:
auth_lookup <- "data/external/LookupAuthorityNames.csv"
auth_lookup_data <- read.csv(auth_lookup)
```

This is a team assignment. Please create a file on your team GitHub repo where 
you answer the challenge, including links to your code, and graphs.

#__Model #1 for question #1:__

###1.	Ask two (2) questions that might help you understand better the dynamics of 
###violence contained on our dataset. Apply one algorithm per question and share 
###your insights from each analysis. [50 pts] Remember: a non-finding is also a 
###finding! It tells you whether a question is worth pursuing further or not.
  
  * Does the military (army and navy) or all other forces combined (other, federal.police,
  state.police, municipal.police, ministerial.police, and afi) seize more of the following:
  clips, cartridges, small arms, and long guns?

###1a	perform the necessary transformations in your data - if any are needed, 
###and explain why you did that

  * Since we want to which understand group, the military or all other forces 
  combined, is more successful in seizing clips, cartridges, small arms, and 
  long guns, two new columns will be created.  The `military` column will combine
  the existing `army` and `navy` columns.  The `all_others` column will combine 
  existing `other`, `federal.police`, `state.police`, `municipal.police`, 
  `ministerial.police`, and `afi` columns.
  
  
```{r}
AllData$military <- transform(AllData$army + AllData$navy) ## if > 1, both groups participated in the event

AllData$all_others <- transform(AllData$other + 
                            AllData$federal.police +
                            AllData$state.police +
                            AllData$municipal.police +
                            AllData$ministerial.police +
                            AllData$afi) ## if > 1, more than one group participated in the event

AllData$both_groups <- ifelse(AllData$military > 0 & AllData$all_others > 0,
                              "yes", ## both AllData$military and AllData$all_others participated in the event
                              "no") 

to_exclude <- AllData$both_groups == "yes" ## rows for which both AllData$military and AllData$all_others participated in the event

AllData_excluded <- AllData[!to_exclude, ] ## create a new dataset with events during which EITHER the `military` OR `all_other` groups participated in each event - no overlap

# creates a random sample to split data into training and testing sets
set.seed(12345) ## for reproducibility 
train <- sample(nrow(AllData_excluded), 2646) ## split approximately in half
training <- AllData_excluded[train, ] 
testing <- AllData_excluded[-train, ]


## ::::::::THIS IS THE PROFESSOR'S CODE - LOOK AT DATA MINING NOTES TO DETERMINE WHICH TYPE OF MODEL TO USE:
#fits model on training set
RandomForestFit <- train(organized.crime.dead ~ organized.crime.wounded +
                             afi + army + navy + federal.police +
                             long.guns.seized+ small.arms.seized + 
                             clips.seized + cartridge.sezied, 
                         data = training,
                         method = "rf",       # defines Random Forests
                         importance = TRUE,   # keeps variable importance
                         prox=TRUE,
                         preProc = c("center", "scale") # standardizes variables
                         )

# gets initial output from model
print(RandomForestFit)

# gets a description of the model
print(RandomForestFit$finalModel)

# gets predictions on the test set
RandomForestPredict <- predict(RandomForestFit, testing)


# :: A quick look to evaluate the model

# gets variable importance
VIMP <- varImp(RandomForestFit)
pdf(outFileName1) 
plot(VIMP)
dev.off()

# plots MSE of the model 
pdf(outFileName2) 
plot(RandomForestFit$finalModel, main = "")
dev.off()
```

###1b	show the output from your analysis in a consumable form
  * 

```{r}

```

###1c	be explicit about the limitations of your anaylisis, due to estimation 
###or to the data itself
  * 
  
```{r}

```

###1d	did you find something interesting? what is that? does your finding 
###suggest this question is worth pursuing further? why or why not?
    * 
  
```{r}

```

###1e	if you did not find something interesting, explain why, and whether 
###there is some additional information that would help in answering your question
  * 
  
```{r}

```

###1f	provide your code, and a single visualization per question that summarizes 
###your finding
  * 
  
```{r}

```

###1g	phrase your finding for each question in two ways:
###1g-1	one sentence that summarizes your insight
  * 

###1g-2	one paragraph that reflects all nuance in your insight
  * 
        
###1h	make sure to also include your code
  
  * The code is included under each quetsion above.
  
#__Model #2 for question #1:__

###1.	Ask two (2) questions that might help you understand better the dynamics of 
###violence contained on our datas et. Apply one algorithm per question and share 
###your insights from each analysis. [50 pts] Remember: a non-finding is also a 
###finding! It tells you whether a question is worth pursuing further or not.
  
  * Brandon
   
```{r}

```

###1a	perform the necessary transformations in your data - if any are needed, 
###and explain why you did that
  * 
  
```{r}

```

###1b	show the output from your analysis in a consumable form
  * 

```{r}

```

###1c	be explicit about the limitations of your anaylisis, due to estimation 
###or to the data itself
  * 
  
```{r}

```

###1d	did you find something interesting? what is that? does your finding 
###suggest this question is worth pursuing further? why or why not?
    * 
  
```{r}

```

###1e	if you did not find something interesting, explain why, and whether 
###there is some additional information that would help in answering your question
  * 
  
```{r}

```

###1f	provide your code, and a single visualization per question that summarizes 
###your finding
  * 
  
```{r}

```

###1g	phrase your finding for each question in two ways:
###1g-1	one sentence that summarizes your insight
  * 

###1g-2	one paragraph that reflects all nuance in your insight
  * 
        
###1h	make sure to also include your code
  
  * The code is included under each quetsion above.
  
#__Model #1 for question #2:__
  
###2.	Formulate two (2) conditional hypotheses that you seek to investigate with 
###the data. One of your hypotheses should condition on two variables (as the 
###example on the slides), and the other should condition on three variables. [50 pts]
    
    * ZACH 

```{r}

```

  
###2a	formulate each one of your hypotheses explicitly in substantive terms (as 
###opposed to statistical terms) using 2-3 lines at most
  * 
  
###2b	show exactly how each one of your hypotheses translates into the marginal 
###effect that you will seek to estimate from the data
  * 
  
###2c	show the output from your analysis in a consumable form
  * 

```{r}

```

###2d	show all your computations to estimate the corresponding marginal effect 
###and its standard error
  * 

```{r}

```

###2e	be explicit in your assumptions
  * 
  
###2f	be explicit in the limitations of your inferences
  * 
  
###2g	phrase your finding for each question in two ways:
###2g-1	one sentence that summarizes your insight
  * 
  
###2g-2	one paragraph that reflects all nuance in your insight
  * 
  
###2h	make sure to also include your code
  
  * The code is included under each quetsion above.

#__Model #1 for question #2:__

###2.	Formulate two (2) conditional hypotheses that you seek to investigate with 
###the data. One of your hypotheses should condition on two variables (as the 
###example on the slides), and the other should condition on three variables. [50 pts]
    
    * ZACH:  

```{r}

```
  
###2a	formulate each one of your hypotheses explicitly in substantive terms (as 
###opposed to statistical terms) using 2-3 lines at most
  * 
  
###2b	show exactly how each one of your hypotheses translates into the marginal 
###effect that you will seek to estimate from the data
  * 
  
###2c	show the output from your analysis in a consumable form
  * 

```{r}

```

###2d	show all your computations to estimate the corresponding marginal effect 
###and its standard error
  * 

```{r}

```

###2e	be explicit in your assumptions
  * 
  
###2f	be explicit in the limitations of your inferences
  * 
  
###2g	phrase your finding for each question in two ways:
###2g-1	one sentence that summarizes your insight
  * 
  
###2g-2	one paragraph that reflects all nuance in your insight
  * 
  
###2h	make sure to also include your code
  
  * The code is included under each quetsion above.
  
  